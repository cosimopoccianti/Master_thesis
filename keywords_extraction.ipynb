{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords extraction\n",
    "\n",
    "The keybert algorithm, which offers speed advantages over LLMs due to the large number of texts to be processed, is used to extract keywords from the abstracts of each paper. Seven keywords are extracted for each paper, because according to some tests carried out, taking the longest abstracts as reference, with less than seven words important information is lost, and with more than seven it becomes redundant. The words may be fewer if the algorithm finds the same word twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cosim\\anaconda3\\envs\\tesi\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#loanding the required libraries\n",
    "import pandas as pd\n",
    "from keybert import KeyBERT\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataset\n",
    "df = pd.read_csv('df_geo_loc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the extractor and the lemmatizer\n",
    "kw_model = KeyBERT()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the keywords, the words that were used to filter the papers are removed, because they certainly predominate in number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_keys = [\"weather\", \"climate\",\"environmental\" ,\"atmospheric\" ,\"meteorological\" ,\"change\" ,\"changes\" ,\"shift\" ,\"shifts\" ,\"alteration\" , \"alterations\" , \"transformation\" , \"transformations\" ,\"extreme\" ,\"severe\" ,\"harsh\" ,\"unusual\" ,\"conditions\" ,\"condition\" ,\"event\" , \"events\" , \"pattern\" , \"patterns\" ,\"phenomena\" , \"episodes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72355/72355 [76:37:28<00:00,  3.81s/it]         \n"
     ]
    }
   ],
   "source": [
    "#extracting the keywords\n",
    "keywords = []\n",
    "for i in tqdm(df['abstract']):\n",
    "    paper_keywords = kw_model.extract_keywords(i, stop_words='english', top_n=7)\n",
    "    words = []\n",
    "    for j in range(len(paper_keywords)):\n",
    "        if paper_keywords[j][0] not in search_keys:\n",
    "            words.append(paper_keywords[j][0])\n",
    "    list_word_lemma = list(np.unique([lemmatizer.lemmatize(i) for i in words]))\n",
    "    keywords.append(list_word_lemma)\n",
    "df['key_words'] = keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the result\n",
    "df.to_csv('df_fe.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
